<!DOCTYPE html>
<html>
<head>
<title>README.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>
<link rel="stylesheet" href="file:///Users/okmt/books/logicalbeat_md.css" type="text/css">
<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<h1 id="%E3%82%B5%E3%83%B3%E3%83%97%E3%83%AB%E3%82%A2%E3%83%97%E3%83%AA%E3%82%B1%E3%83%BC%E3%82%B7%E3%83%A7%E3%83%B3%E6%A6%82%E8%A6%81">サンプルアプリケーション概要</h1>
<p>SPAのフロントエンドとREST APIのバックエンドに分かれる</p>
<p>また、スケジュール起動されるバッチアプリケーションも存在する</p>
<h2 id="api%E3%82%A2%E3%83%97%E3%83%AA%E3%82%B1%E3%83%BC%E3%82%B7%E3%83%A7%E3%83%B3">APIアプリケーション</h2>
<p>Spring Bootを利用している</p>
<p>APIエンドポイントはRegionAPI、LocationAPI、HealthAPIをSpring MVCで作成している</p>
<p>データベースアクセスはSpring Data JPAを使っている</p>
<p><code>@CrossOrigin</code>を利用してCORSをAPI側で設定している</p>
<h2 id="%E3%83%95%E3%83%AD%E3%83%B3%E3%83%88%E3%82%A8%E3%83%B3%E3%83%89%E3%82%A2%E3%83%97%E3%83%AA%E3%82%B1%E3%83%BC%E3%82%B7%E3%83%A7%E3%83%B3">フロントエンドアプリケーション</h2>
<p>Nuxt.jsを利用している</p>
<h2 id="%E3%83%90%E3%83%83%E3%83%81%E3%82%A2%E3%83%97%E3%83%AA%E3%82%B1%E3%83%BC%E3%82%B7%E3%83%A7%E3%83%B3">バッチアプリケーション</h2>
<p>CommandLineRunnderを用いてSpring Bootを用いて開発している</p>
<p>バッチアプリケーションではSpring CloudというライブラリでS3にアクセスしている</p>
<h1 id="eks%E6%A7%8B%E7%AF%89%E3%81%AB%E5%88%A9%E7%94%A8%E3%81%99%E3%82%8B%E3%83%84%E3%83%BC%E3%83%AB-eksctl">EKS構築に利用するツール eksctl</h1>
<p>EKSクラスターの構築および管理を行うためのOSSコマンドラインツール</p>
<p>VPC、サブネット、セキュリティグループなどを一括して構築することができる</p>
<p>本手順ではVPNでのベースリソースは先に作成しておき、EKSクラスターを構築するときにそれらのリソースIDを指定することにする</p>
<h2 id="%E3%83%99%E3%83%BC%E3%82%B9%E3%83%AA%E3%82%BD%E3%83%BC%E3%82%B9%E3%81%AE%E4%BD%9C%E6%88%90">ベースリソースの作成</h2>
<p>以下のツールをインストールしておく</p>
<ul>
<li>AWS CLI</li>
<li>eksctl</li>
<li>kubectl</li>
</ul>
<p>eks-work-base.yaml を用意してCloudFormationに食わせる</p>
<pre class="hljs"><code><div>AWSTemplateFormatVersion: '2010-09-09'

Parameters:
  ClusterBaseName:
    Type: String
    Default: eks-work

  TargetRegion:
    Type: String
    Default: ap-northeast-1

  AvailabilityZone1:
    Type: String
    Default: ap-northeast-1a

  AvailabilityZone2:
    Type: String
    Default: ap-northeast-1c

  AvailabilityZone3:
    Type: String
    Default: ap-northeast-1d

  VpcBlock:
    Type: String
    Default: 192.168.0.0/16

  WorkerSubnet1Block:
    Type: String
    Default: 192.168.0.0/24

  WorkerSubnet2Block:
    Type: String
    Default: 192.168.1.0/24

  WorkerSubnet3Block:
    Type: String
    Default: 192.168.2.0/24

Resources:
  EksWorkVPC:
    Type: AWS::EC2::VPC
    Properties:
      CidrBlock: !Ref VpcBlock
      EnableDnsSupport: true
      EnableDnsHostnames: true
      Tags:
        - Key: Name
          Value: !Sub ${ClusterBaseName}-VPC

  WorkerSubnet1:
    Type: AWS::EC2::Subnet
    Properties:
      AvailabilityZone: !Ref AvailabilityZone1
      CidrBlock: !Ref WorkerSubnet1Block
      VpcId: !Ref EksWorkVPC
      MapPublicIpOnLaunch: true
      Tags:
        - Key: Name
          Value: !Sub ${ClusterBaseName}-WorkerSubnet1

  WorkerSubnet2:
    Type: AWS::EC2::Subnet
    Properties:
      AvailabilityZone: !Ref AvailabilityZone2
      CidrBlock: !Ref WorkerSubnet2Block
      VpcId: !Ref EksWorkVPC
      MapPublicIpOnLaunch: true
      Tags:
        - Key: Name
          Value: !Sub ${ClusterBaseName}-WorkerSubnet2

  WorkerSubnet3:
    Type: AWS::EC2::Subnet
    Properties:
      AvailabilityZone: !Ref AvailabilityZone3
      CidrBlock: !Ref WorkerSubnet3Block
      VpcId: !Ref EksWorkVPC
      MapPublicIpOnLaunch: true
      Tags:
        - Key: Name
          Value: !Sub ${ClusterBaseName}-WorkerSubnet3

  InternetGateway:
    Type: AWS::EC2::InternetGateway

  VPCGatewayAttachment:
    Type: AWS::EC2::VPCGatewayAttachment
    Properties:
      InternetGatewayId: !Ref InternetGateway
      VpcId: !Ref EksWorkVPC

  WorkerSubnetRouteTable:
    Type: AWS::EC2::RouteTable
    Properties:
      VpcId: !Ref EksWorkVPC
      Tags:
        - Key: Name
          Value: !Sub ${ClusterBaseName}-WorkerSubnetRouteTable

  WorkerSubnetRoute:
    Type: AWS::EC2::Route
    Properties:
      RouteTableId: !Ref WorkerSubnetRouteTable
      DestinationCidrBlock: 0.0.0.0/0
      GatewayId: !Ref InternetGateway

  WorkerSubnet1RouteTableAssociation:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      SubnetId: !Ref WorkerSubnet1
      RouteTableId: !Ref WorkerSubnetRouteTable

  WorkerSubnet2RouteTableAssociation:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      SubnetId: !Ref WorkerSubnet2
      RouteTableId: !Ref WorkerSubnetRouteTable

  WorkerSubnet3RouteTableAssociation:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      SubnetId: !Ref WorkerSubnet3
      RouteTableId: !Ref WorkerSubnetRouteTable

Outputs:
  VPC:
    Value: !Ref EksWorkVPC

  WorkerSubnets:
    Value: !Join
      - &quot;,&quot;
      - [!Ref WorkerSubnet1, !Ref WorkerSubnet2, !Ref WorkerSubnet3]

  RouteTable:
    Value: !Ref WorkerSubnetRouteTable
</div></code></pre>
<p>リソースの作成が完了するとCREATE_IN_PROGRESSからCREATE_COMPLETEに変わる</p>
<p><img src="eks-work-base.png" alt="eks-work-base"></p>
<p>上記操作でVPCも作成済み</p>
<p><img src="vpc.png" alt="vpc"></p>
<h2 id="eks%E3%82%AF%E3%83%A9%E3%82%B9%E3%82%BF%E3%83%BC%E6%A7%8B%E7%AF%89">EKSクラスター構築</h2>
<p>ベースリソースの情報はCloudFormationの出力タブで確認可能</p>
<p><img src="eks-work-base2.png" alt="eks-work-base2"></p>
<p>WorkerSubnetsの値をメモる</p>
<p>下記のeksctlを実行する</p>
<pre class="hljs"><code><div>$ worker_subnets='上記でメモった値'
$ eksctl create cluster \
--vpc-public-subnets ${worker_subnets} \
--name eks-worker-cluster \
--version 1.25 \
--nodegroup-name eks-worker-nodegroup \
--node-type t2.small \
--nodes 2 \
--nodes-min 2 \
--nodes-max 5
</div></code></pre>
<p>出力する</p>
<pre class="hljs"><code><div>2023-06-28 15:52:47 [ℹ]  eksctl version 0.146.0
2023-06-28 15:52:47 [ℹ]  using region ap-northeast-1
2023-06-28 15:52:48 [✔]  using existing VPC (vpc-049f0f8f507a9f442) and subnets (private:map[] public:map[ap-northeast-1a:{subnet-07dfb8aef1bfc103b ap-northeast-1a 192.168.0.0/24 0 } ap-northeast-1c:{subnet-0cfacecd339eda649 ap-northeast-1c 192.168.1.0/24 0 } ap-northeast-1d:{subnet-008c65a949a8ced94 ap-northeast-1d 192.168.2.0/24 0 }])
2023-06-28 15:52:48 [!]  custom VPC/subnets will be used; if resulting cluster doesn't function as expected, make sure to review the configuration of VPC/subnets
2023-06-28 15:52:48 [ℹ]  nodegroup &quot;eks-worker-nodegroup&quot; will use &quot;&quot; [AmazonLinux2/1.25]
2023-06-28 15:52:48 [ℹ]  using Kubernetes version 1.25
2023-06-28 15:52:48 [ℹ]  creating EKS cluster &quot;eks-worker-cluster&quot; in &quot;ap-northeast-1&quot; region with managed nodes
2023-06-28 15:52:48 [ℹ]  will create 2 separate CloudFormation stacks for cluster itself and the initial managed nodegroup
2023-06-28 15:52:48 [ℹ]  if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=ap-northeast-1 --cluster=eks-worker-cluster'
2023-06-28 15:52:48 [ℹ]  Kubernetes API endpoint access will use default of {publicAccess=true, privateAccess=false} for cluster &quot;eks-worker-cluster&quot; in &quot;ap-northeast-1&quot;
2023-06-28 15:52:48 [ℹ]  CloudWatch logging will not be enabled for cluster &quot;eks-worker-cluster&quot; in &quot;ap-northeast-1&quot;
2023-06-28 15:52:48 [ℹ]  you can enable it with 'eksctl utils update-cluster-logging --enable-types={SPECIFY-YOUR-LOG-TYPES-HERE (e.g. all)} --region=ap-northeast-1 --cluster=eks-worker-cluster'
2023-06-28 15:52:48 [ℹ]  
2 sequential tasks: { create cluster control plane &quot;eks-worker-cluster&quot;, 
    2 sequential sub-tasks: { 
        wait for control plane to become ready,
        create managed nodegroup &quot;eks-worker-nodegroup&quot;,
    } 
}
2023-06-28 15:52:48 [ℹ]  building cluster stack &quot;eksctl-eks-worker-cluster-cluster&quot;
2023-06-28 15:52:49 [ℹ]  deploying stack &quot;eksctl-eks-worker-cluster-cluster&quot;
2023-06-28 15:53:19 [ℹ]  waiting for CloudFormation stack &quot;eksctl-eks-worker-cluster-cluster&quot;
2023-06-28 15:53:49 [ℹ]  waiting for CloudFormation stack &quot;eksctl-eks-worker-cluster-cluster&quot;
2023-06-28 15:54:49 [ℹ]  waiting for CloudFormation stack &quot;eksctl-eks-worker-cluster-cluster&quot;
2023-06-28 15:55:50 [ℹ]  waiting for CloudFormation stack &quot;eksctl-eks-worker-cluster-cluster&quot;
2023-06-28 15:56:50 [ℹ]  waiting for CloudFormation stack &quot;eksctl-eks-worker-cluster-cluster&quot;
2023-06-28 15:57:50 [ℹ]  waiting for CloudFormation stack &quot;eksctl-eks-worker-cluster-cluster&quot;
2023-06-28 15:58:50 [ℹ]  waiting for CloudFormation stack &quot;eksctl-eks-worker-cluster-cluster&quot;
2023-06-28 15:59:51 [ℹ]  waiting for CloudFormation stack &quot;eksctl-eks-worker-cluster-cluster&quot;
2023-06-28 16:00:51 [ℹ]  waiting for CloudFormation stack &quot;eksctl-eks-worker-cluster-cluster&quot;
2023-06-28 16:01:51 [ℹ]  waiting for CloudFormation stack &quot;eksctl-eks-worker-cluster-cluster&quot;
2023-06-28 16:03:54 [ℹ]  building managed nodegroup stack &quot;eksctl-eks-worker-cluster-nodegroup-eks-worker-nodegroup&quot;
2023-06-28 16:03:55 [ℹ]  deploying stack &quot;eksctl-eks-worker-cluster-nodegroup-eks-worker-nodegroup&quot;
2023-06-28 16:03:55 [ℹ]  waiting for CloudFormation stack &quot;eksctl-eks-worker-cluster-nodegroup-eks-worker-nodegroup&quot;
2023-06-28 16:04:25 [ℹ]  waiting for CloudFormation stack &quot;eksctl-eks-worker-cluster-nodegroup-eks-worker-nodegroup&quot;
2023-06-28 16:04:58 [ℹ]  waiting for CloudFormation stack &quot;eksctl-eks-worker-cluster-nodegroup-eks-worker-nodegroup&quot;
2023-06-28 16:06:25 [ℹ]  waiting for CloudFormation stack &quot;eksctl-eks-worker-cluster-nodegroup-eks-worker-nodegroup&quot;
2023-06-28 16:08:01 [ℹ]  waiting for CloudFormation stack &quot;eksctl-eks-worker-cluster-nodegroup-eks-worker-nodegroup&quot;
2023-06-28 16:08:01 [ℹ]  waiting for the control plane to become ready
2023-06-28 16:08:02 [✔]  saved kubeconfig as &quot;/Users/okmt/.kube/config&quot;
2023-06-28 16:08:02 [ℹ]  no tasks
2023-06-28 16:08:02 [✔]  all EKS cluster resources for &quot;eks-worker-cluster&quot; have been created
2023-06-28 16:08:02 [ℹ]  nodegroup &quot;eks-worker-nodegroup&quot; has 2 node(s)
2023-06-28 16:08:02 [ℹ]  node &quot;ip-192-168-0-46.ap-northeast-1.compute.internal&quot; is ready
2023-06-28 16:08:02 [ℹ]  node &quot;ip-192-168-1-107.ap-northeast-1.compute.internal&quot; is ready
2023-06-28 16:08:02 [ℹ]  waiting for at least 2 node(s) to become ready in &quot;eks-worker-nodegroup&quot;
2023-06-28 16:08:02 [ℹ]  nodegroup &quot;eks-worker-nodegroup&quot; has 2 node(s)
2023-06-28 16:08:02 [ℹ]  node &quot;ip-192-168-0-46.ap-northeast-1.compute.internal&quot; is ready
2023-06-28 16:08:02 [ℹ]  node &quot;ip-192-168-1-107.ap-northeast-1.compute.internal&quot; is ready
2023-06-28 16:08:02 [ℹ]  kubectl command should work with &quot;/Users/okmt/.kube/config&quot;, try 'kubectl get nodes'
2023-06-28 16:08:02 [✔]  EKS cluster &quot;eks-worker-cluster&quot; in &quot;ap-northeast-1&quot; region is ready
</div></code></pre>
<p>このコマンド20分くらいかかるんだけど、びっくりするよね。</p>
<p>CloudFormationの進捗はUIでも確認可能</p>
<p><img src="eksctl-eks-worker-cluster-cluster.png" alt="eksctl-eks-worker-cluster-cluster"></p>
<p>上記コマンドで以下2つを作成できる</p>
<ul>
<li>EKSクラスター</li>
<li>ワーカーノード</li>
</ul>
<h2 id="kubeconfig%E3%81%AE%E8%A8%AD%E5%AE%9A">kubeconfigの設定</h2>
<p>kubeconfigはk8sクライアントのkubectlが利用する設定ファイルで接続先のk8sクラスターの接続情報を保持している</p>
<p>eksctlはEKSクラスター構築の中でkubeconfigファイルを自動的に更新してくれる</p>
<p>${USER}/.kube/config に配置されている</p>
<pre class="hljs"><code><div>$ kubectl config get-contexts
CURRENT   NAME                                                 CLUSTER                                       AUTHINFO                                             NAMESPACE
*         awscli@eks-worker-cluster.ap-northeast-1.eksctl.io   eks-worker-cluster.ap-northeast-1.eksctl.io   awscli@eks-worker-cluster.ap-northeast-1.eksctl.io

$ kubectl get nodes
NAME                                               STATUS   ROLES    AGE    VERSION
ip-192-168-0-46.ap-northeast-1.compute.internal    Ready    &lt;none&gt;   9m6s   v1.25.9-eks-0a21954
ip-192-168-1-107.ap-northeast-1.compute.internal   Ready    &lt;none&gt;   9m4s   v1.25.9-eks-0a21954
</div></code></pre>
<h2 id="eks%E3%82%AF%E3%83%A9%E3%82%B9%E3%82%BF%E3%83%BC%E3%81%AE%E5%8B%95%E4%BD%9C%E7%A2%BA%E8%AA%8D">EKSクラスターの動作確認</h2>
<p>02_nginx_k8s.yaml</p>
<pre class="hljs"><code><div>apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod
  labels:
    app: nginx-app
spec:
  containers:
  - name: nginx-container
    image: nginx
    ports:
      - containerPort: 80
</div></code></pre>
<p>Podを作成</p>
<pre class="hljs"><code><div>$ kubectl apply -f 02_nginx_k8s.yaml
pod/nginx-pod created
</div></code></pre>
<p>Podの情報を取得</p>
<pre class="hljs"><code><div>$ kubectl get pods
NAME        READY   STATUS    RESTARTS   AGE
nginx-pod   1/1     Running   0          80s
</div></code></pre>
<p>ポートフォワーディングする</p>
<pre class="hljs"><code><div>$ kubectl port-forward nginx-pod 8080:80
Forwarding from 127.0.0.1:8080 -&gt; 80
Forwarding from [::1]:8080 -&gt; 80
</div></code></pre>
<p>これで http://localhost:8080 を開くとEKSクラスター上でnginxが立ち上がっている</p>
<p><img src="nginx.png" alt="nginx"></p>
<pre class="hljs"><code><div>$ kubectl delete pod nginx-pod
pod &quot;nginx-pod&quot; deleted
</div></code></pre>
<h2 id="%E3%83%87%E3%83%BC%E3%82%BF%E3%83%99%E3%83%BC%E3%82%B9%E3%81%AE%E3%82%BB%E3%83%83%E3%83%88%E3%82%A2%E3%83%83%E3%83%97">データベースのセットアップ</h2>
<p>データベース環境もCloudFormationから作成する</p>
<p>但し、作成時に下記のパラメータを設定すること</p>
<ul>
<li>EksWorkVPC: プルダウンメニュー 選択肢のうち kes-work-VPC が含まれる行</li>
<li>OpeServerRouteTable: CloudFormation -&gt; eks-work-base -&gt; 出力 -&gt; RouteTable rtb-0xxxxxxxxxxxxxx</li>
</ul>
<p>IAMも作成できるようにチェックをいれる</p>
<pre class="hljs"><code><div>AWSTemplateFormatVersion: &quot;2010-09-09&quot;

Parameters:
  EksWorkVPC:
    Type: AWS::EC2::VPC::Id

  WorkerSubnet1Block:
    Type: String
    Default: 192.168.0.0/24

  WorkerSubnet2Block:
    Type: String
    Default: 192.168.1.0/24

  WorkerSubnet3Block:
    Type: String
    Default: 192.168.2.0/24

  OpeSubnetBlock:
    Type: String
    Default: 192.168.5.0/24

  OpeServerRouteTable:
    Type: String

  ClusterBaseName:
    Type: String
    Default: eks-work

  TargetRegion:
    Type: String
    Default: ap-northeast-1

  AvailabilityZone1:
    Type: String
    Default: ap-northeast-1a

  AvailabilityZone2:
    Type: String
    Default: ap-northeast-1c

  AvailabilityZone3:
    Type: String
    Default: ap-northeast-1d

  RdsSubnet1Block:
    Type: String
    Default: 192.168.3.0/24

  RdsSubnet2Block:
    Type: String
    Default: 192.168.4.0/24

  OpeServerInstanceType:
    Type: String
    Default: t2.micro

  OpeServerImageId:
    Type: String
    Default: ami-00d101850e971728d # ap-northeast-1, SSD, Amazon Linux 2

  OpeServerVolumeSize:
    Type: Number
    Default: 8

Resources:
  RdsSubnet1:
    Type: AWS::EC2::Subnet
    Properties:
      AvailabilityZone: !Ref AvailabilityZone1
      CidrBlock: !Ref RdsSubnet1Block
      VpcId: !Ref EksWorkVPC

  RdsSubnet2:
    Type: AWS::EC2::Subnet
    Properties:
      AvailabilityZone: !Ref AvailabilityZone2
      CidrBlock: !Ref RdsSubnet2Block
      VpcId: !Ref EksWorkVPC

  RdsSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Security Group for RDS
      VpcId: !Ref EksWorkVPC

  RdsIngressPostgreSQLWorker1:
    Type: AWS::EC2::SecurityGroupIngress
    Properties:
      GroupId: !Ref RdsSecurityGroup
      CidrIp: !Ref WorkerSubnet1Block
      IpProtocol: tcp
      FromPort: 5432
      ToPort: 5432

  RdsIngressPostgreSQLWorker2:
    Type: AWS::EC2::SecurityGroupIngress
    Properties:
      GroupId: !Ref RdsSecurityGroup
      CidrIp: !Ref WorkerSubnet2Block
      IpProtocol: tcp
      FromPort: 5432
      ToPort: 5432

  RdsIngressPostgreSQLWorker3:
    Type: AWS::EC2::SecurityGroupIngress
    Properties:
      GroupId: !Ref RdsSecurityGroup
      CidrIp: !Ref WorkerSubnet3Block
      IpProtocol: tcp
      FromPort: 5432
      ToPort: 5432

  RdsIngressPostgreSQLOpe:
    Type: AWS::EC2::SecurityGroupIngress
    Properties:
      GroupId: !Ref RdsSecurityGroup
      CidrIp: !Ref OpeSubnetBlock
      IpProtocol: tcp
      FromPort: 5432
      ToPort: 5432

  EksWorkDB:
    Type: &quot;AWS::RDS::DBInstance&quot;
    Properties:
      DBInstanceIdentifier: eks-work-db
      Engine: postgres
      EngineVersion: 14.8
      DBInstanceClass: db.t3.micro
      AllocatedStorage: 30
      StorageType: gp2
      DBName: eksworkdb
      MasterUsername: !Join ['', ['{{resolve:secretsmanager:', !Ref RdsMasterSecret, ':SecretString:username}}' ]]
      MasterUserPassword: !Join ['', ['{{resolve:secretsmanager:', !Ref RdsMasterSecret, ':SecretString:password}}' ]]
      DBSubnetGroupName: !Ref EksWorkDBSubnetGroup
      PubliclyAccessible: false
      MultiAZ: false
      PreferredBackupWindow: 18:00-18:30
      PreferredMaintenanceWindow: sat:19:00-sat:19:30
      AutoMinorVersionUpgrade: false
      DBParameterGroupName: !Ref EksWorkDBParameterGroup
      VPCSecurityGroups:
        - !Ref RdsSecurityGroup
      CopyTagsToSnapshot: true
      BackupRetentionPeriod: 7
      DeletionProtection: false

  RdsMasterSecret:
    Type: AWS::SecretsManager::Secret
    Properties:
      GenerateSecretString:
        SecretStringTemplate: '{&quot;username&quot;: &quot;eksdbadmin&quot;}'
        GenerateStringKey: 'password'
        PasswordLength: 16
        ExcludeCharacters: '&quot;@/\'''
      Name: RdsMasterSecret

  RdsUserSecret:
    Type: AWS::SecretsManager::Secret
    Properties:
      GenerateSecretString:
        SecretStringTemplate: '{&quot;username&quot;: &quot;mywork&quot;}'
        GenerateStringKey: 'password'
        PasswordLength: 16
        ExcludeCharacters: '&quot;@/\''{}#%&amp;*&lt;&gt;[]^`|'
      Name: RdsUserSecret

  RdsSecretAttachment:
    Type: AWS::SecretsManager::SecretTargetAttachment
    Properties:
      SecretId: !Ref RdsMasterSecret
      TargetId: !Ref EksWorkDB
      TargetType: AWS::RDS::DBInstance

  EksWorkDBSubnetGroup:
    Type: AWS::RDS::DBSubnetGroup
    Properties:
      DBSubnetGroupName: subnet-group-eks-work-db
      DBSubnetGroupDescription: &quot;DB Subnet Group&quot;
      SubnetIds:
        - !Ref RdsSubnet1
        - !Ref RdsSubnet2

  EksWorkDBParameterGroup:
    Type: AWS::RDS::DBParameterGroup
    Properties:
      Family: postgres14
      Description: Parameter Group for PostgreSQL 14.8

  OpeServerSubnet:
    Type: AWS::EC2::Subnet
    Properties:
      AvailabilityZone: !Ref AvailabilityZone1
      CidrBlock: !Ref OpeSubnetBlock
      VpcId: !Ref EksWorkVPC

  OpeServerSubnetRouteTableAssociation:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      SubnetId: !Ref OpeServerSubnet
      RouteTableId: !Ref OpeServerRouteTable

  OpeServerSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Security Group for Ope
      VpcId: !Ref EksWorkVPC

  RdsIngressPostgreSQLFromOpe:
    Type: AWS::EC2::SecurityGroupIngress
    Properties:
      GroupId: !Ref RdsSecurityGroup
      SourceSecurityGroupId: !Ref OpeServerSecurityGroup
      IpProtocol: tcp
      FromPort: 5432
      ToPort: 5432

  OpeServerRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub ${ClusterBaseName}-OpeServerRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AmazonEC2RoleforSSM
      Path: /
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - ec2.amazonaws.com
            Action:
              - sts:AssumeRole

  OpeServerInstanceProfile:
    Type: AWS::IAM::InstanceProfile
    Properties:
      Path: /
      Roles:
        - !Ref OpeServerRole

  OpeServerEIP:
    Type: AWS::EC2::EIP
    Properties:
      Domain: vpc

  OpeServerEIPAssociation:
    Type: AWS::EC2::EIPAssociation
    Properties:
      AllocationId: !GetAtt OpeServerEIP.AllocationId
      InstanceId: !Ref OpeServerInstance

  OpeServerInstance:
    Type: AWS::EC2::Instance
    Properties:
      InstanceType: !Ref OpeServerInstanceType
      ImageId: !Ref OpeServerImageId
      BlockDeviceMappings:
        - DeviceName: /dev/xvda
          Ebs:
            VolumeType: gp2
            VolumeSize: 8
            DeleteOnTermination: true
      NetworkInterfaces:
        - SubnetId: !Ref OpeServerSubnet
          AssociatePublicIpAddress: false
          GroupSet:
            - !Ref OpeServerSecurityGroup
          DeviceIndex: 0
          DeleteOnTermination: true
      DisableApiTermination: false
      IamInstanceProfile: !Ref OpeServerInstanceProfile
      UserData:
        Fn::Base64:
          !Sub |
          #!/bin/bash
          set -o xtrace
          yum install -y https://s3.amazonaws.com/ec2-downloads-windows/SSMAgent/latest/linux_amd64/amazon-ssm-agent.rpm
          /opt/aws/bin/cfn-signal --exit-code $? \
                   --stack  ${AWS::StackName} \
                   --resource NodeGroup  \
                   --region ${AWS::Region}

Outputs:
  RDSEndpoint:
    Value: !GetAtt EksWorkDB.Endpoint.Address

</div></code></pre>
<p>postgresのバージョンが古かったから失敗するので調べておくこと</p>
<pre class="hljs"><code><div>$ aws rds describe-db-engine-versions --default-only --engine postgres
</div></code></pre>
<pre class="hljs"><code><div>$ aws rds describe-orderable-db-instance-options --engine postgres --engine-version 14.8 --query 'OrderableDBInstanceOptions[].[DBInstanceClass,StorageType,Engine,EngineVersion]' --output table --region ap-northeast-1
------------------------------------------------------
|         DescribeOrderableDBInstanceOptions         |
+-------------------+-----------+-----------+--------+
|  db.m5.12xlarge   |  gp2      |  postgres |  14.8  |
|  db.m5.12xlarge   |  gp3      |  postgres |  14.8  |
|  db.m5.12xlarge   |  io1      |  postgres |  14.8  |
|  db.m5.12xlarge   |  standard |  postgres |  14.8  |
...
|  db.t3.large      |  gp2      |  postgres |  14.8  |
|  db.t3.large      |  gp3      |  postgres |  14.8  |
|  db.t3.large      |  io1      |  postgres |  14.8  |
|  db.t3.large      |  standard |  postgres |  14.8  |
|  db.t3.medium     |  gp2      |  postgres |  14.8  |
|  db.t3.medium     |  gp3      |  postgres |  14.8  |
|  db.t3.medium     |  io1      |  postgres |  14.8  |
|  db.t3.medium     |  standard |  postgres |  14.8  |
|  db.t3.micro      |  gp2      |  postgres |  14.8  |
|  db.t3.micro      |  gp3      |  postgres |  14.8  |
|  db.t3.micro      |  io1      |  postgres |  14.8  |
|  db.t3.micro      |  standard |  postgres |  14.8  |
|  db.t3.small      |  gp2      |  postgres |  14.8  |
|  db.t3.small      |  gp3      |  postgres |  14.8  |
|  db.t3.small      |  io1      |  postgres |  14.8  |
...

</div></code></pre>
<p>Systems Manager -&gt; セッションマネージャーでセッションを開始する</p>
<pre class="hljs"><code><div>$ sudo yum install -y git
$ sudo amazon-linux-extras install -y postgresql10
</div></code></pre>
<p>データベースの以下の値を確認</p>
<ul>
<li>エンドポイントアドレス: CloudFormation 出力 RDSEndpoint</li>
<li>管理者用パスワード: Secrets Manager RdsMasterSecret</li>
<li>アプリケーション用データベースユーザのパスワード: Secrets Manager RdsUserSecret</li>
</ul>
<pre class="hljs"><code><div>$ createuser -d -U eksdbadmin -P -h eks-work-db.cl1b9exbukrt.ap-northeast-1.rds.amazonaws.com mywork
</div></code></pre>
<pre class="hljs"><code><div>$ createdb -U mywork -h eks-work-db.cl1b9exbukrt.ap-northeast-1.rds.amazonaws.com -E UTF8 myworkdb
</div></code></pre>
<p>接続</p>
<pre class="hljs"><code><div>$ psql -U mywork -h eks-work-db.cl1b9exbukrt.ap-northeast-1.rds.amazonaws.com myworkdb
</div></code></pre>
<p>10_ddl.sql</p>
<pre class="hljs"><code><div>CREATE TABLE region
(
  region_id          SERIAL PRIMARY KEY,
  region_name        VARCHAR(100) NOT NULL,
  creation_timestamp TIMESTAMP    NOT NULL
);

CREATE TABLE location
(
  location_id   BIGSERIAL PRIMARY KEY,
  location_name VARCHAR(200) NOT NULL,
  region_id     BIGINT       NOT NULL,
  note          TEXT,
  FOREIGN KEY (region_id) REFERENCES region (region_id)
);

CREATE TABLE batch_processing
(
  batch_name VARCHAR(20) PRIMARY KEY,
  last_execution_date_time TIMESTAMP
);

CREATE TABLE batch_processing_file
(
  batch_processing_file_id BIGSERIAL PRIMARY KEY,
  batch_name VARCHAR(20) NOT NULL,
  file_name VARCHAR(300) NOT NULL
);
</div></code></pre>
<p>20_insert_sample_data.sql</p>
<pre class="hljs"><code><div>-- REGION
INSERT INTO region (region_name, creation_timestamp)
VALUES ('北海道', current_timestamp);

INSERT INTO region (region_name, creation_timestamp)
VALUES ('東北', current_timestamp);

INSERT INTO region (region_name, creation_timestamp)
VALUES ('関東', current_timestamp);

INSERT INTO region (region_name, creation_timestamp)
VALUES ('中部', current_timestamp);

INSERT INTO region (region_name, creation_timestamp)
VALUES ('近畿', current_timestamp);

INSERT INTO region (region_name, creation_timestamp)
VALUES ('中国', current_timestamp);

INSERT INTO region (region_name, creation_timestamp)
VALUES ('四国', current_timestamp);

INSERT INTO region (region_name, creation_timestamp)
VALUES ('九州', current_timestamp);

INSERT INTO region (region_name, creation_timestamp)
VALUES ('沖縄', current_timestamp);

-- LOCATION
INSERT INTO location (location_name, region_id, note)
VALUES ('美ら海水族館', (SELECT region_id FROM region WHERE region_name = '沖縄'),
  '沖縄の代表的な水族館で、ジンベエザメをはじめ、様々な沖縄の海の生き物を見ることができます。');

INSERT INTO location (location_name, region_id, note)
VALUES ('首里城', (SELECT region_id FROM region WHERE region_name = '沖縄'),
  '琉球王朝の王城で、世界遺産の1つです。');

-- BATCH_PROCESSING
INSERT INTO batch_processing (batch_name)
values ('SAMPLE_APP_BATCH');
</div></code></pre>
<p>初期データ投入</p>
<pre class="hljs"><code><div>myworkdb=&gt; \i 10_ddl.sql
CREATE TABLE
CREATE TABLE
CREATE TABLE
CREATE TABLE
myworkdb=&gt; \i 20_insert_sample_data.sql
INSERT 0 1
INSERT 0 1
INSERT 0 1
INSERT 0 1
INSERT 0 1
INSERT 0 1
INSERT 0 1
INSERT 0 1
INSERT 0 1
INSERT 0 1
INSERT 0 1
INSERT 0 1
</div></code></pre>
<h2 id="api%E3%81%AEcontainer-build%E3%81%A8push">APIのcontainer buildとpush</h2>
<p>Corretto をインストールした後、JAVA_HOMEの環境変数を設定</p>
<pre class="hljs"><code><div>export JAVA_HOME=$(/usr/libexec/java_home -v 11)
</div></code></pre>
<p>以下のコマンドでソースコードから実行可能jarを作成する</p>
<pre class="hljs"><code><div>$ ./gradlew clean build
</div></code></pre>
<p>実行可能ファイルは <code>backend-app-1.0.0.jar</code> という名前で配置している</p>
<p>Dockerfileがあるディレクトリでbuildコマンドを実行する</p>
<pre class="hljs"><code><div>$ docker build --platform amd64 -t k8sbook/backend-app:1.1.0 --build-arg JAR_FILE=backend-app-1.0.0.jar .
</div></code></pre>
<p>buildされた後の成果物</p>
<pre class="hljs"><code><div>$ docker images | grep &quot;k8sbook&quot;
k8sbook/backend-app                                  1.0.0       9d000b75dbae   4 days ago      516MB
</div></code></pre>
<p>Container Resistoryの準備してアップロードする</p>
<p>まず、Amazon ECRを用意する</p>
<p><img src="./repository.png" alt="ECR"></p>
<p>作成したもの</p>
<p><img src="./repository2.png" alt="ECR2"></p>
<p>ログインする</p>
<pre class="hljs"><code><div>$ aws ecr get-login-password --region ap-northeast-1 | docker login --username AWS --password-stdin 761624429622.dkr.ecr.ap-northeast-1.amazonaws.com
Login Succeeded
</div></code></pre>
<p>dockerのtag付けをする</p>
<pre class="hljs"><code><div>$ docker tag k8sbook/backend-app:1.0.0 761624429622.dkr.ecr.ap-northeast-1.amazonaws.com/k8sbook/backend-app:1.0.0
</div></code></pre>
<p>pushする</p>
<pre class="hljs"><code><div>$ docker push 761624429622.dkr.ecr.ap-northeast-1.amazonaws.com/k8sbook/backend-app:1.0.0
The push refers to repository [761624429622.dkr.ecr.ap-northeast-1.amazonaws.com/k8sbook/backend-app]
18e12aace9aa: Pushed 
f30dc1e9505b: Pushed 
e3bc576ca49a: Pushed 
19bf53d200d5: Pushed 
1.0.0: digest: sha256:dbc5d282f2cc8fb4e7027a1aa4a14d9e4054c2d46a19db7d08581f6e3b7a7084 size: 1161
</div></code></pre>
<p><img src="./container.png" alt="container"></p>
<h2 id="eks%E3%82%AF%E3%83%A9%E3%82%B9%E3%82%BF%E3%83%BC%E3%81%B8%E3%81%AE-api%E3%82%A2%E3%83%97%E3%83%AA%E3%82%B1%E3%83%BC%E3%82%B7%E3%83%A7%E3%83%B3%E3%81%AE%E3%83%87%E3%83%97%E3%83%AD%E3%82%A4">EKSクラスターへの APIアプリケーションのデプロイ</h2>
<ul>
<li>Namespaceの作成</li>
<li>kubeconfigへのNamespaceの反映</li>
<li>データベース接続用のSecretの登録</li>
<li>APIアプリケーションのデプロイ</li>
<li>APIアプリケーションの公開</li>
</ul>
<p>20_create_namespace_k8s.yaml を用意</p>
<pre class="hljs"><code><div>apiVersion: v1
kind: Namespace
metadata:
  name: eks-work
</div></code></pre>
<p>kubectl を適用する</p>
<pre class="hljs"><code><div>$ kubectl apply -f 20_create_namespace_k8s.yaml
namespace/eks-work created
</div></code></pre>
<pre class="hljs"><code><div>$ kubectl config get-contexts
CURRENT   NAME                                                 CLUSTER                                       AUTHINFO                                             NAMESPACE
*         awscli@eks-worker-cluster.ap-northeast-1.eksctl.io   eks-worker-cluster.ap-northeast-1.eksctl.io   awscli@eks-worker-cluster.ap-northeast-1.eksctl.io 
</div></code></pre>
<p>上記コマンドのAUTHINFOとCLUSTER列を使用する</p>
<pre class="hljs"><code><div>$ AUTHINFO='awscli@eks-worker-cluster.ap-northeast-1.eksctl.io'
$ CLUSTER='eks-worker-cluster.ap-northeast-1.eksctl.io'
$ kubectl config set-context eks-work --cluster $CLUSTER --user $AUTHINFO --namespace eks-work
Context &quot;eks-work&quot; created.
[2023-07-04T05:28:54] okmt@mba:~/plays/aws/k8s_on_aws $ kubectl config use-context eks-work
Switched to context &quot;eks-work&quot;.
[2023-07-04T05:29:00] okmt@mba:~/plays/aws/k8s_on_aws $ kubectl config get-contexts
CURRENT   NAME                                                 CLUSTER                                       AUTHINFO                                             NAMESPACE
          awscli@eks-worker-cluster.ap-northeast-1.eksctl.io   eks-worker-cluster.ap-northeast-1.eksctl.io   awscli@eks-worker-cluster.ap-northeast-1.eksctl.io   
*         eks-work                                             eks-worker-cluster.ap-northeast-1.eksctl.io   awscli@eks-worker-cluster.ap-northeast-1.eksctl.io   eks-work
</div></code></pre>
<p>データベース接続用Secret作成</p>
<ul>
<li>RDSエンドポイントアドレス（CloudFormationの出力タブから取得）: 'eks-work-db.cl1b9exbukrt.ap-northeast-1.rds.amazonaws.com'</li>
<li>アプリケーション用データベースユーザのパスワード（Secret Managerの画面から取得）: ';1cg3bt0FAL)qiOC'</li>
</ul>
<pre class="hljs"><code><div>$ ENDPOINT='eks-work-db.cl1b9exbukrt.ap-northeast-1.rds.amazonaws.com' \
PASSWORD=';1cg3bt0FAL)qiOC' \
envsubst &lt; k8sbook/eks-env/21_db_config_k8s.yaml.template | \
kubectl apply -f - 

secret/db-config created
</div></code></pre>
<p>[補足] secretの削除方法</p>
<pre class="hljs"><code><div>$ kubectl get secrets
NAME        TYPE     DATA   AGE
db-config   Opaque   3      12h

$ kubectl delete secret db-config
secret &quot;db-config&quot; deleted
</div></code></pre>
<p>APIアプリケーションのデプロイ</p>
<pre class="hljs"><code><div>$ ECR_HOST='761624429622.dkr.ecr.ap-northeast-1.amazonaws.com' \ 
envsubst &lt; k8sbook/eks-env/22_deployment_backend-app_k8s.yaml.template | \
kubectl apply -f -

deployment.apps/backend-app created
deployment.apps/backend-app configured
</div></code></pre>
<p>コンポーネントの作成がうまく行かなかったとき</p>
<pre class="hljs"><code><div>$ kubectl get all
NAME                               READY   STATUS             RESTARTS   AGE
pod/backend-app-6c7d4794c8-ljsg4   0/1     InvalidImageName   0          3m12s
pod/backend-app-6c7d4794c8-xjrgt   0/1     InvalidImageName   0          3m12s

NAME                          READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/backend-app   0/2     2            0           3m13s

NAME                                     DESIRED   CURRENT   READY   AGE
replicaset.apps/backend-app-6c7d4794c8   2         2         0       3m13s
</div></code></pre>
<p>[補足] podの削除方法</p>
<pre class="hljs"><code><div>$ kubectl delete -f k8sbook/eks-env/22_deployment_backend-app_k8s.yaml.template 
deployment.apps &quot;backend-app&quot; deleted
</div></code></pre>
<p>コンポーネントの作成がうまく行った時は以下のようになる</p>
<pre class="hljs"><code><div>$ kubectl get all
NAME                              READY   STATUS    RESTARTS   AGE
pod/backend-app-89b68f9fc-gj94v   1/1     Running   0          88s
pod/backend-app-89b68f9fc-mrbn2   1/1     Running   0          96s

NAME                          READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/backend-app   2/2     2            2           30m

NAME                                    DESIRED   CURRENT   READY   AGE
replicaset.apps/backend-app-89b68f9fc   2         2         2       30m
</div></code></pre>
<p>APIアプリケーションの公開</p>
<pre class="hljs"><code><div>$ kubectl apply -f k8sbook/eks-env/23_service_backend-app_k8s.yaml 
service/backend-app-service created

$ kubectl get all
NAME                              READY   STATUS    RESTARTS   AGE
pod/backend-app-89b68f9fc-gj94v   1/1     Running   0          10m
pod/backend-app-89b68f9fc-mrbn2   1/1     Running   0          10m

NAME                          TYPE           CLUSTER-IP     EXTERNAL-IP                                                                  PORT(S)          AGE
service/backend-app-service   LoadBalancer   10.100.61.85   a159eb0af247f42e994342dab57d432a-47984927.ap-northeast-1.elb.amazonaws.com   8080:30893/TCP   4s

NAME                          READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/backend-app   2/2     2            2           38m

NAME                                    DESIRED   CURRENT   READY   AGE
replicaset.apps/backend-app-89b68f9fc   2         2         2       38m
</div></code></pre>
<p>EXTERNAL-IP列には「elb.amazonaws.com」で終わるアドレスが出力されている</p>
<p>ロードバランサーが出来上がっている</p>
<p><img src="./ec2lb.png" alt="el"></p>
<p>インスタンスタブを確認する</p>
<p><img src="./ec2lbinstance.png" alt="el"></p>
<p>EXTERNAL-IPの値を指定して動作確認できる</p>
<pre class="hljs"><code><div>$ curl -s http://a159eb0af247f42e994342dab57d432a-47984927.ap-northeast-1.elb.amazonaws.com:8080/health
{&quot;status&quot;:&quot;OK&quot;}
</div></code></pre>
<h2 id="%E3%83%95%E3%83%AD%E3%83%B3%E3%83%88%E3%82%A8%E3%83%B3%E3%83%89%E3%82%A2%E3%83%97%E3%83%AA%E3%82%B1%E3%83%BC%E3%82%B7%E3%83%A7%E3%83%B3%E3%81%AE%E3%83%93%E3%83%AB%E3%83%89%E3%81%A8%E3%83%87%E3%83%97%E3%83%AD%E3%82%A4">フロントエンドアプリケーションのビルドとデプロイ</h2>
<ul>
<li>フロントエンドアプリケーションのビルド</li>
<li>S3バケットとCloudFrontディストリビューションを作成</li>
<li>フロントエンドコンテンツをS3を配置</li>
<li>CloudFront経由でアクセスし、アプリケーションの動作確認を行う</li>
</ul>
<pre class="hljs"><code><div>$ cd k8sbook/frontend-app; npm install
</div></code></pre>
<pre class="hljs"><code><div>$ kubectl get all
NAME                              READY   STATUS    RESTARTS   AGE
pod/backend-app-89b68f9fc-gj94v   1/1     Running   0          133m
pod/backend-app-89b68f9fc-mrbn2   1/1     Running   0          133m

NAME                          TYPE           CLUSTER-IP     EXTERNAL-IP                                                                  PORT(S)          AGE
service/backend-app-service   LoadBalancer   10.100.61.85   a159eb0af247f42e994342dab57d432a-47984927.ap-northeast-1.elb.amazonaws.com   8080:30893/TCP   123m

NAME                          READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/backend-app   2/2     2            2           161m

NAME                                    DESIRED   CURRENT   READY   AGE
replicaset.apps/backend-app-89b68f9fc   2         2         2       161m
</div></code></pre>
<p>service/ から始まる行のEXTERNAL-IP列の値をメモる</p>
<pre class="hljs"><code><div>$ BASE_URL=http://a159eb0af247f42e994342dab57d432a-47984927.ap-northeast-1.elb.amazonaws.com:8080 npm run build
</div></code></pre>
<p>CloudFormationは「スタックを作成」-&gt;「新しいリソースを使用」を押し、30_s3_cloudfront_cfn.yamlを選択して作成</p>
<pre class="hljs"><code><div>$ aws s3 ls
2021-08-21 10:21:47 cf-templates-sppp0xkpzhes-ap-northeast-1
2023-07-05 01:03:15 eks-work-frontendks
2022-03-20 23:30:27 mattermost-bucket-okmt1230z
</div></code></pre>
<pre class="hljs"><code><div>$ aws s3 sync dist s3://eks-work-frontendks --delete --include &quot;*&quot; --acl public-read
upload failed: dist/.nojekyll to s3://eks-work-frontendks/.nojekyll An error occurred (AccessControlListNotSupported) when calling the PutObject operation: The bucket does not allow ACLs
upload failed: dist/_nuxt/70ce022.js to s3://eks-work-frontendks/_nuxt/70ce022.js An error occurred (AccessControlListNotSupported) when calling the PutObject operation: The bucket does not allow ACLs
upload failed: dist/_nuxt/44844a7.js to s3://eks-work-frontendks/_nuxt/44844a7.js An error occurred (AccessControlListNotSupported) when calling the PutObject operation: The bucket does not allow ACLs
upload failed: dist/favicon.ico to s3://eks-work-frontendks/favicon.ico An error occurred (AccessControlListNotSupported) when calling the PutObject operation: The bucket does not allow ACLs
upload failed: dist/_nuxt/8bc0000.js to s3://eks-work-frontendks/_nuxt/8bc0000.js An error occurred (AccessControlListNotSupported) when calling the PutObject operation: The bucket does not allow ACLs
upload failed: dist/_nuxt/613ab16.js to s3://eks-work-frontendks/_nuxt/613ab16.js An error occurred (AccessControlListNotSupported) when calling the PutObject operation: The bucket does not allow ACLs
upload failed: dist/index.html to s3://eks-work-frontendks/index.html An error occurred (AccessControlListNotSupported) when calling the PutObject operation: The bucket does not allow ACLs
upload failed: dist/_nuxt/LICENSES to s3://eks-work-frontendks/_nuxt/LICENSES An error occurred (AccessControlListNotSupported) when calling the PutObject operation: The bucket does not allow ACLs
upload failed: dist/_nuxt/239fd50.js to s3://eks-work-frontendks/_nuxt/239fd50.js An error occurred (AccessControlListNotSupported) when calling the PutObject operation: The bucket does not allow ACLs
upload failed: dist/200.html to s3://eks-work-frontendks/200.html An error occurred (AccessControlListNotSupported) when calling the PutObject operation: The bucket does not allow ACLs
upload failed: dist/_nuxt/9cf1e70.js to s3://eks-work-frontendks/_nuxt/9cf1e70.js An error occurred (AccessControlListNotSupported) when calling the PutObject operation: The bucket does not allow ACLs
upload failed: dist/v.png to s3://eks-work-frontendks/v.png An error occurred (AccessControlListNotSupported) when calling the PutObject operation: The bucket does not allow ACLs
upload failed: dist/regionDetail/index.html to s3://eks-work-frontendks/regionDetail/index.html An error occurred (AccessControlListNotSupported) when calling the PutObject operation: The bucket does not allow ACLs
</div></code></pre>
<p>MFAを設定する</p>
<pre class="hljs"><code><div> $ aws sts get-session-token --serial-number arn:aws:iam::761624429622:mfa/oldest-iphone --token-code 136672 --profile mfa
{
    &quot;Credentials&quot;: {
        &quot;AccessKeyId&quot;: &quot;ASIA3CVC2QQ3LJ5GVQP6&quot;,
        ...
        ...
        &quot;Expiration&quot;: &quot;2023-07-05T17:56:12+00:00&quot;
    }
}
</div></code></pre>
<p>public readは一旦外す</p>
<pre class="hljs"><code><div>$ aws s3 sync dist s3://eks-work-frontendks --delete --include &quot;*&quot;
upload: dist/.nojekyll to s3://eks-work-frontendks/.nojekyll
upload: dist/index.html to s3://eks-work-frontendks/index.html    
upload: dist/regionDetail/index.html to s3://eks-work-frontendks/regionDetail/index.html
upload: dist/_nuxt/LICENSES to s3://eks-work-frontendks/_nuxt/LICENSES
upload: dist/_nuxt/613ab16.js to s3://eks-work-frontendks/_nuxt/613ab16.js
upload: dist/_nuxt/9cf1e70.js to s3://eks-work-frontendks/_nuxt/9cf1e70.js
upload: dist/favicon.ico to s3://eks-work-frontendks/favicon.ico  
upload: dist/200.html to s3://eks-work-frontendks/200.html        
upload: dist/_nuxt/44844a7.js to s3://eks-work-frontendks/_nuxt/44844a7.js
upload: dist/v.png to s3://eks-work-frontendks/v.png              
upload: dist/_nuxt/70ce022.js to s3://eks-work-frontendks/_nuxt/70ce022.js
upload: dist/_nuxt/239fd50.js to s3://eks-work-frontendks/_nuxt/239fd50.js
upload: dist/_nuxt/8bc0000.js to s3://eks-work-frontendks/_nuxt/8bc0000.js
</div></code></pre>
<p>CloudFront ディストリビューションのキャッシュ無効化させる</p>
<pre class="hljs"><code><div>$ aws cloudfront create-invalidation --distribution-id E2UDWV4TAPJ2KO --path &quot;/*&quot;
{
    &quot;Location&quot;: &quot;https://cloudfront.amazonaws.com/2020-05-31/distribution/E2UDWV4TAPJ2KO/invalidation/IAHB1QMXDMHI9PJ9PBMXCESIGW&quot;,
    &quot;Invalidation&quot;: {
        &quot;Id&quot;: &quot;IAHB1QMXDMHI9PJ9PBMXCESIGW&quot;,
        &quot;Status&quot;: &quot;InProgress&quot;,
        &quot;CreateTime&quot;: &quot;2023-07-05T06:17:49.765000+00:00&quot;,
        &quot;InvalidationBatch&quot;: {
            &quot;Paths&quot;: {
                &quot;Quantity&quot;: 1,
                &quot;Items&quot;: [
                    &quot;/*&quot;
                ]
            },
            &quot;CallerReference&quot;: &quot;cli-1688537868-444282&quot;
        }
    }
}
</div></code></pre>
<p>EKSのeks-work-frontendの出力の値を見て、URLを確認</p>
<p><img src="./eks-frontend-output.png" alt="frontend-outout"></p>
<p>http://d9gtzk6ywkybn.cloudfront.net</p>
<p><img src="./view.png" alt="view"></p>
<p>パブリックアクセス可能になっているのでセキュリティには注意してね</p>
<p><img src="./publicaccess.png" alt="public"></p>
<p>ちょっとダサいかも</p>
<h2 id="%E3%83%90%E3%83%83%E3%83%81%E3%82%A2%E3%83%97%E3%83%AA%E3%82%B1%E3%83%BC%E3%82%B7%E3%83%A7%E3%83%B3%E3%81%AE%E3%83%93%E3%83%AB%E3%83%89%E3%81%A8%E3%83%87%E3%83%97%E3%83%AD%E3%82%A4">バッチアプリケーションのビルドとデプロイ</h2>
<p>k8sのCronJobという仕組みを利用する</p>
<p>バッチアプリケーションのビルド</p>
<pre class="hljs"><code><div>$ cd k8sbook/batch-app; ./gradlew clean build
Starting a Gradle Daemon (subsequent builds will be faster)

BUILD SUCCESSFUL in 28s
14 actionable tasks: 13 executed, 1 up-to-date
</div></code></pre>
<p>コンテナイメージの作成</p>
<pre class="hljs"><code><div>$ docker build --platform amd64 -t k8sbook/batch-app:1.0.0 --build-arg JAR_TILE=./k8sbook/batch-app/build/libs/batch-app-1.0.0.jar .

[+] Building 14.8s (8/8) FINISHED                                                                                                                                           
 =&gt; [internal] load build definition from Dockerfile                                                                                                                   0.0s
 =&gt; =&gt; transferring dockerfile: 37B                                                                                                                                    0.0s
 =&gt; [internal] load .dockerignore                                                                                                                                      0.0s
 =&gt; =&gt; transferring context: 2B                                                                                                                                        0.0s
 =&gt; [internal] load metadata for docker.io/library/amazoncorretto:11                                                                                                   1.4s
 =&gt; [internal] load build context                                                                                                                                      6.7s
 =&gt; =&gt; transferring context: 605.14MB                                                                                                                                  6.6s
 =&gt; [1/3] FROM docker.io/library/amazoncorretto:11@sha256:97297e012ef1adc7722f4525184b039423955fb8625fa33b2aaabf1b7021252b                                             0.0s
 =&gt; CACHED [2/3] RUN ln -sf /usr/share/zoneinfo/Japan /etc/localtime                                                                                                   0.0s
 =&gt; [3/3] COPY  app.jar                                                                                                                                                4.0s
 =&gt; exporting to image                                                                                                                                                 2.5s
 =&gt; =&gt; exporting layers                                                                                                                                                2.5s
 =&gt; =&gt; writing image sha256:45ed4f2c5457c878086b1816f210537459c0e41dc19fdc604cc18d02ed90e954                                                                           0.0s
 =&gt; =&gt; naming to docker.io/k8sbook/batch-app:1.0.0                                                                                                                     0.0s
Use 'docker scan' to run Snyk tests against images to find vulnerabilities and learn how to fix them
</div></code></pre>
<p>docker images</p>
<pre class="hljs"><code><div>$ docker images
REPOSITORY                   TAG       IMAGE ID       CREATED          SIZE
k8sbook/batch-app            1.0.0     b7cedad92304   19 seconds ago   1.05GB
</div></code></pre>
<p>ECRレポジトリの作成</p>
<p><img src="./batch-app-repository.png" alt="batch-app-repository"></p>
<pre class="hljs"><code><div>$ docker tag k8sbook/batch-app:1.0.0 761624429622.dkr.ecr.ap-northeast-1.amazonaws.com/k8sbook/batch-app:1.0.0
$ docker push 761624429622.dkr.ecr.ap-northeast-1.amazonaws.com/k8sbook/batch-app:1.0.0
</div></code></pre>
<p><img src="./batch-app-repository2.png" alt="batch-app-repository2"></p>
<p>バッチアプリケーションが使用するS3バケットを作成する</p>
<p>CloudFormationで<code>40_s3_batch_cfn.yaml</code>を食わせて作成</p>
<p><img src="./eks-work-batch.png" alt="eks-work-batch"></p>
<p>S3バケットが作成されたら、バッチアプリケーションの設定値を格納するConfigMapを作成する</p>
<p><code>41_config_map_batch_k8s.yaml.template</code></p>
<pre class="hljs"><code><div>apiVersion: v1
kind: ConfigMap
metadata:
  name: batch-app-config
data:
  bucket-name: eks-work-batch-${BUCKET_SUFFIX}
  folder-name: locationData
  batch-run: &quot;true&quot;
  aws-region: ap-northeast-1
</div></code></pre>
<p>ConfigMapの作成</p>
<pre class="hljs"><code><div>$ BUCKET_SUFFIX=ks \
envsubst &lt; 41_config_map_batch_k8s.yaml.template | kubectl apply -f -
configmap/batch-app-config created
</div></code></pre>
<pre class="hljs"><code><div>$ kubectl get configmap batch-app-config
NAME               DATA   AGE
batch-app-config   4      9m58s

$ kubectl get configmap batch-app-config -o yaml
apiVersion: v1
data:
  aws-region: ap-northeast-1
  batch-run: &quot;true&quot;
  bucket-name: eks-work-batch-ks
  folder-name: locationData
kind: ConfigMap
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {&quot;apiVersion&quot;:&quot;v1&quot;,&quot;data&quot;:{&quot;aws-region&quot;:&quot;ap-northeast-1&quot;,&quot;batch-run&quot;:&quot;true&quot;,&quot;bucket-name&quot;:&quot;eks-work-batch-ks&quot;,&quot;folder-name&quot;:&quot;locationData&quot;},&quot;kind&quot;:&quot;ConfigMap&quot;,&quot;metadata&quot;:{&quot;annotations&quot;:{},&quot;name&quot;:&quot;batch-app-config&quot;,&quot;namespace&quot;:&quot;eks-work&quot;}}
  creationTimestamp: &quot;2023-07-06T10:21:59Z&quot;
  name: batch-app-config
  namespace: eks-work
  resourceVersion: &quot;1615192&quot;
  uid: 4e45d394-0d99-4faa-99e1-fd690ac4434e
</div></code></pre>
<p>S3アクセス用アクセスキーの取得とSecretの作成</p>
<p>AWS System Managerのパラメータストア</p>
<p><img src="./parameterstore.png" alt="parameterstore"></p>
<p>アクセスキーを確認できる</p>
<p><code>42_batch_secrets_k8s.yaml.template</code>を用意</p>
<pre class="hljs"><code><div>apiVersion: v1
kind: Secret
type: Opaque
metadata:
  name: batch-secret-config
stringData:
  aws-accesskey: ${AWS_ACCESSKEY}
  aws-secretkey: ${AWS_SECRETKEY}
</div></code></pre>
<pre class="hljs"><code><div>AWS_ACCESSKEY=~ \
AWS_SECRETKEY=~ \
envsubst &lt; 42_batch_secrets_k8s.yaml.template | kubectl apply -f -
secret/batch-secret-config created
</div></code></pre>
<p>バッチアプリケーションへの入力ファイルの配置</p>
<pre class="hljs"><code><div>$ aws s3 sync k8sbook/batch-app/sample_data/normal s3:eks-work-batch-ks/locationData --delete --include &quot;*&quot; --acl public-read
upload failed: k8sbook/batch-app/sample_data/normal/sample_location1.csv to s3://eks-work-batch-ks/locationData/sample_location1.csv An error occurred (AccessControlListNotSupported) when calling the PutObject operation: The bucket does not allow ACLs
upload failed: k8sbook/batch-app/sample_data/normal/sample_location2.csv to s3://eks-work-batch-ks/locationData/sample_location2.csv An error occurred (AccessControlListNotSupported) when calling the PutObject operation: The bucket does not allow ACLs
</div></code></pre>
<p>後からパブリックアクセスできるようにするワ</p>
<pre class="hljs"><code><div>$ aws s3 sync k8sbook/batch-app/sample_data/normal s3://eks-work-batch-ks/locationData --delete --include &quot;*&quot;
upload: k8sbook/batch-app/sample_data/normal/sample_location1.csv to s3://eks-work-batch-ks/locationData/sample_location1.csv
upload: k8sbook/batch-app/sample_data/normal/sample_location2.csv to s3://eks-work-batch-ks/locationData/sample_location2.csv
</div></code></pre>
<p>バッチアプリケーションのデプロイ</p>
<p><code>43_cronjob_k8s.yaml.template</code></p>
<pre class="hljs"><code><div>apiVersion: batch/v1
kind: CronJob
metadata:
  name: batch-app
spec:
  schedule: &quot;*/5 * * * *&quot; # min hour day-of-month month day-of-week
  jobTemplate:
    spec:
      template:
        spec:
          containers:
            - name: batch-app
              image: ${ECR_HOST}/k8sbook/batch-app:1.0.0
              imagePullPolicy: Always
              env:
                - name: DB_URL
                  valueFrom:
                    secretKeyRef:
                      key: db-url
                      name: db-config
                - name: DB_USERNAME
                  valueFrom:
                    secretKeyRef:
                      key: db-username
                      name: db-config
                - name: DB_PASSWORD
                  valueFrom:
                    secretKeyRef:
                      key: db-password
                      name: db-config
                - name: CLOUD_AWS_CREDENTIALS_ACCESSKEY
                  valueFrom:
                    secretKeyRef:
                      key: aws-accesskey
                      name: batch-secret-config
                - name: CLOUD_AWS_CREDENTIALS_SECRETKEY
                  valueFrom:
                    secretKeyRef:
                      key: aws-secretkey
                      name: batch-secret-config
                - name: CLOUD_AWS_REGION_STATIC
                  valueFrom:
                    configMapKeyRef:
                      key: aws-region
                      name: batch-app-config
                - name: SAMPLE_APP_BATCH_BUCKET_NAME
                  valueFrom:
                    configMapKeyRef:
                      key: bucket-name
                      name: batch-app-config
                - name: SAMPLE_APP_BATCH_FOLDER_NAME
                  valueFrom:
                    configMapKeyRef:
                      key: folder-name
                      name: batch-app-config
                - name: SAMPLE_APP_BATCH_RUN
                  valueFrom:
                    configMapKeyRef:
                      key: batch-run
                      name: batch-app-config
          restartPolicy: OnFailure
</div></code></pre>
<pre class="hljs"><code><div>$ ECR_HOST=761624429622.dkr.ecr.ap-northeast-1.amazonaws.com \
envsubst &lt; 43_cronjob_k8s.yaml.template | \
kubectl apply -f -
cronjob.batch/batch-app created
</div></code></pre>
<p>kubectl get all</p>
<pre class="hljs"><code><div>$ kubectl get all
NAME                              READY   STATUS    RESTARTS   AGE
pod/backend-app-89b68f9fc-gj94v   1/1     Running   0          45h
pod/backend-app-89b68f9fc-mrbn2   1/1     Running   0          45h

NAME                          TYPE           CLUSTER-IP     EXTERNAL-IP                                                                  PORT(S)          AGE
service/backend-app-service   LoadBalancer   10.100.61.85   a159eb0af247f42e994342dab57d432a-47984927.ap-northeast-1.elb.amazonaws.com   8080:30893/TCP   45h

NAME                          READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/backend-app   2/2     2            2           45h

NAME                                    DESIRED   CURRENT   READY   AGE
replicaset.apps/backend-app-89b68f9fc   2         2         2       45h

NAME                      SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
cronjob.batch/batch-app   */5 * * * *   False     0        &lt;none&gt;          29s
</div></code></pre>
<p>5の倍数の時だけ以下のようにPodが増える</p>
<pre class="hljs"><code><div>$ kubectl get all
NAME                              READY   STATUS    RESTARTS      AGE
pod/backend-app-89b68f9fc-gj94v   1/1     Running   0             45h
pod/backend-app-89b68f9fc-mrbn2   1/1     Running   0             45h
pod/batch-app-28144025-zxh7h      0/1     Error     2 (30s ago)   58s
</div></code></pre>
<p>こけているけどまぁいっか</p>
<p>p.129の内容をまとめたい</p>

</body>
</html>
